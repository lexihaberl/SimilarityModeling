{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4c0604",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d64689b-26a5-4720-8ab8-6b7c41294322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import librosa\n",
    "\n",
    "from PIL import Image\n",
    "import IPython.display as ipd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from tqdm import tqdm\n",
    "from utils import io, train\n",
    "from utils import feature_engineering as fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92a0533-d581-454e-9bcb-0c94d117e38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "videos_folder = \"../data_full/videos\"\n",
    "gt_folder = \"../data_full/gt_annotations\"\n",
    "temp_folder = \"../data_full/tmp/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_names, video_paths, gt_df = io.get_init_vars(gt_folder, videos_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9a2e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5432362",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: use straight lines for the detection of Balcony Gentlemens, since Balcony has a very characteristical straight lines. Can we use Hough Transform in Sim1? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import cv2 as cv\n",
    "img_path = '../data/frames/Muppets-03-04-03/frame_09404.jpg' \n",
    "src = cv.imread(img_path, cv.IMREAD_GRAYSCALE)    \n",
    "dst = cv.Canny(src, 50, 200, None, 3)\n",
    "    \n",
    "cdst = cv.cvtColor(dst, cv.COLOR_GRAY2BGR)\n",
    "cdstP = np.copy(cdst)\n",
    "\n",
    "lines = cv.HoughLinesWithAccumulator(dst, 1, np.pi / 180, 200, None, 0, 0)\n",
    "\n",
    "if lines is not None:\n",
    "    for i in range(0, len(lines)):\n",
    "        rho = lines[i][0][0]\n",
    "        theta = lines[i][0][1]\n",
    "        a = math.cos(theta)\n",
    "        b = math.sin(theta)\n",
    "        x0 = a * rho\n",
    "        y0 = b * rho\n",
    "        pt1 = (int(x0 + 1000*(-b)), int(y0 + 1000*(a)))\n",
    "        pt2 = (int(x0 - 1000*(-b)), int(y0 - 1000*(a)))\n",
    "        cv.line(cdst, pt1, pt2, (0,0,255), 3, cv.LINE_AA)\n",
    "        if i == 3:\n",
    "            break\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(src)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(cdst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line_feat_dict = {}\n",
    "\n",
    "# for ep in episode_names:\n",
    "#     cap = io.load_video(video_paths[ep])\n",
    "#     frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#     line_num_list = []  \n",
    "#     max_votes_list = []\n",
    "    \n",
    "#     for i in range(frame_count):\n",
    "#         _, frame = cap.read()\n",
    "\n",
    "#         dst = cv2.Canny(frame, 50, 200, None, 3)\n",
    "#         cdst = cv2.cvtColor(dst, cv2.COLOR_GRAY2BGR)\n",
    "#         cdstP = np.copy(cdst)\n",
    "#         lines = cv2.HoughLinesWithAccumulator(dst, 0.5, np.pi / 180, 200, None, 0, 0)\n",
    "        \n",
    "#         line_feat = len(lines) if lines is not None else 0\n",
    "#         max_votes = lines[0][0][2] if lines is not None else 0\n",
    "\n",
    "#         line_num_list.append(line_feat)\n",
    "#         max_votes_list.append(max_votes)\n",
    "\n",
    "#     line_feat_dict[ep] = {}\n",
    "#     line_feat_dict[ep]['num_lines'] =  np.array(line_num_list)[1:]\n",
    "#     line_feat_dict[ep]['max_votes'] =  np.array(max_votes_list)[1:]\n",
    "\n",
    "#     del line_num_list\n",
    "#     del max_votes_list\n",
    "#     cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute blobs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radius_col = 2\n",
    "n_max = 3\n",
    "debug = False\n",
    "white_params = {\n",
    "    'mu_v': 200, \n",
    "    'sigma_v': 40, \n",
    "    'mu_h': 30, \n",
    "    'sigma_h': 30\n",
    "}\n",
    "\n",
    "radien_feat_dict = {}\n",
    "\n",
    "for ep in episode_names:\n",
    "    cap = io.load_video(video_paths[ep])\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    green_radien_list = []  \n",
    "    white_radien_list = []\n",
    "    \n",
    "    for i in tqdm(range(frame_count)):\n",
    "        _, image = cap.read()\n",
    "\n",
    "        image_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "        image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        image_rbg = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if debug:\n",
    "            plt.imshow(image_rbg)\n",
    "\n",
    "        sigma = 7\n",
    "        green_yellow = np.uint8([[[255*0.85, 255, 0]]])\n",
    "        hsv_green_yellow = cv2.cvtColor(green_yellow, cv2.COLOR_RGB2HSV)\n",
    "        mu = hsv_green_yellow[0, 0, 0]\n",
    "\n",
    "        blob_list = fe.detect_blob(image_hsv, sigma, mu, debug=debug)\n",
    "        blob_radius = blob_list[:, radius_col]\n",
    "        biggest_radien_green = blob_radius[np.argsort(blob_radius)[-n_max:]]\n",
    "\n",
    "        keypoints = fe.detect_blob_cv(image_gray, image, image_hsv, debug=debug, **white_params)\n",
    "        k_radien = np.array([k.size for k in keypoints])\n",
    "        biggest_radien_eyes = k_radien[np.argsort(k_radien)[-n_max:]]\n",
    "\n",
    "    radien_feat_dict[ep] = {}\n",
    "    radien_feat_dict[ep]['num_lines'] =  np.array(green_radien_list)[1:]\n",
    "    radien_feat_dict[ep]['max_votes'] =  np.array(white_radien_list)[1:]\n",
    "\n",
    "    del green_radien_list\n",
    "    del white_radien_list\n",
    "    cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(radien_feat_dict, open(\"../data/features/radien_feat_dict.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '../data/frames/Muppets-03-04-03/frame_00000.jpg'\n",
    "image = cv2.imread(img_path)\n",
    "image_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "image_rbg = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "if debug:\n",
    "    plt.imshow(image_rbg)\n",
    "\n",
    "sigma = 7\n",
    "green_yellow = np.uint8([[[255*0.85, 255, 0]]])\n",
    "hsv_green_yellow = cv2.cvtColor(green_yellow, cv2.COLOR_RGB2HSV)\n",
    "mu = hsv_green_yellow[0, 0, 0]\n",
    "\n",
    "# blob_list = fe.detect_blob(image_hsv, sigma, mu, debug=debug)\n",
    "# blob_radius = blob_list[:, radius_col]\n",
    "# biggest_radien_green = blob_radius[np.argsort(blob_radius)[-n_max:]]\n",
    "\n",
    "# keypoints = fe.detect_blob_cv(image_gray, image, image_hsv, debug=debug, **white_params)\n",
    "# k_radien = np.array([k.size for k in keypoints])\n",
    "# biggest_radien_eyes = k_radien[np.argsort(k_radien)[-n_max:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import MiniBatchKMeans\n",
    "# from scipy.cluster.vq import whiten\n",
    "# n_clusters = 5\n",
    "# batch_size = 2048\n",
    "\n",
    "# hue_feat_list = {}\n",
    "\n",
    "# for ep in ['Muppets-03-04-03']:\n",
    "#     cap = io.load_video(video_paths[ep])\n",
    "#     frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#     hue_list = []\n",
    "\n",
    "#     for i in tqdm(range(frame_count)):\n",
    "#         _, image = cap.read()\n",
    "\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "#         hue = image[:, :, 0].flatten()\n",
    "#         hue_std = hue.std()\n",
    "#         if hue_std == 0:\n",
    "#             hue_list.append([[0, 0] for _ in range(n_clusters)])\n",
    "#             continue\n",
    "#         scaled_hue = np.expand_dims(whiten(hue), -1)\n",
    "\n",
    "#         km = MiniBatchKMeans(n_clusters = n_clusters, batch_size=batch_size, n_init='auto').fit(scaled_hue)\n",
    "#         cluster_centers = km.cluster_centers_\n",
    "\n",
    "#         dominant_hues = []\n",
    "#         for cluster_center in cluster_centers:\n",
    "#             hue_scaled = cluster_center[0]\n",
    "        \n",
    "#             # Convert each standardized value to scaled value\n",
    "#             dominant_hues.append(\n",
    "#                 hue_scaled * hue_std,\n",
    "#             )\n",
    "\n",
    "#         hues = np.asarray(dominant_hues, dtype='uint8')\n",
    "\n",
    "#         percentage = np.asarray(np.unique(km.labels_, return_counts = True)[1], dtype='float32')\n",
    "#         percentage = percentage/(image.shape[0]*image.shape[1])\n",
    "\n",
    "#         if len(percentage) < n_clusters:\n",
    "#             percentage = np.append(percentage, np.zeros(n_clusters - len(percentage)))\n",
    "\n",
    "#         dom = [[percentage[ix], hues[ix]] for ix in range(km.n_clusters)]\n",
    "#         dominance = sorted(dom, key=lambda x:x[0], reverse=True)\n",
    "\n",
    "#         hue_list.append(dominance)\n",
    "\n",
    "#         del km\n",
    "\n",
    "#     hue_feat_list[ep] = {}\n",
    "#     hue_feat_list[ep]['hue_list'] =  np.array(hue_list)[1:]\n",
    "\n",
    "#     del hue_list\n",
    "#     cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hue_feat_list = pickle.load(open(\"../data/features/hue_feat_list_full.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute audio features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0be812",
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_dfs = []\n",
    "for ep in episode_names:\n",
    "    rec, sr = librosa.load(video_paths[ep], sr=None)\n",
    "\n",
    "    frame_size_ms = 400\n",
    "    hop_length = int(1/25 * sr)\n",
    "    frame_length = int(frame_size_ms / 1000 * sr)\n",
    "    \n",
    "    desired_len = len(gt_df[gt_df.episode==ep])\n",
    "    zcr = librosa.feature.zero_crossing_rate(y=rec, frame_length=frame_length, hop_length=hop_length)\n",
    "    zcr = np.pad(zcr, pad_width=((0, 0), (0, desired_len - zcr.shape[1]))).flatten()\n",
    "\n",
    "    rms = librosa.feature.rms(y=rec, frame_length=frame_length, hop_length=hop_length)\n",
    "    rms = np.pad(rms, pad_width=((0, 0), (0, desired_len - rms.shape[1]))).flatten()\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(y=rec, sr=sr, n_fft=frame_length, hop_length=hop_length)\n",
    "    mfcc = np.pad(mfcc, pad_width=((0, 0), (0, desired_len - mfcc.shape[1])))\n",
    "\n",
    "    ep_df = pd.DataFrame()\n",
    "\n",
    "    ep_df['zcr'] = zcr\n",
    "    ep_df['rms'] = rms\n",
    "    \n",
    "    for i in range(mfcc.shape[0]):\n",
    "        ep_df[f'mfcc_{i}'] = mfcc[i]\n",
    "    \n",
    "    ep_df['episode'] = ep\n",
    "    ep_dfs.append(ep_df)\n",
    "\n",
    "feat_df = pd.concat(ep_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute feature dicts:\n",
    "- Dominant Color\n",
    "- Line Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_feat_dict = fe.get_dominant_color(episode_names, video_paths, path_to_save=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add dominant color to the dataframe: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in episode_names:\n",
    "    for i in range(color_feat_dict[ep].shape[1]):\n",
    "        feat_df.loc[feat_df['episode'] == ep, f'dc_{i}'] = color_feat_dict[ep][:, i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add line features to the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in episode_names:\n",
    "    for line_feat_type in ['num_lines', 'max_votes']:\n",
    "        feat_df.loc[feat_df['episode'] == ep, line_feat_type] = line_feat_dict[ep][line_feat_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in episode_names:\n",
    "    current_hue_feat_list = hue_feat_list[ep]['hue_list']\n",
    "    for i, col_name in enumerate(['percentage', 'hue']):\n",
    "        for k in range(current_hue_feat_list.shape[1]):\n",
    "            feat_df.loc[feat_df['episode'] == ep, f'{col_name}_{k}'] = current_hue_feat_list[:, k, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715656ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to listen to audio\n",
    "# ipd.Audio(rec, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b31404",
   "metadata": {},
   "source": [
    "# Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3906d3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6122ea4",
   "metadata": {},
   "source": [
    "## Inner CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kermit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87be6682",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_configs = [\n",
    "    {\n",
    "        'train_cols': ['zcr'],\n",
    "        'target_col': 'Kermit',\n",
    "        # 'model_name': 'RF',\n",
    "        # 'model': RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "        'model_name': 'DT',\n",
    "        'model': DecisionTreeClassifier(random_state=42)\n",
    "    },\n",
    "    {\n",
    "        'train_cols': ['rms'],\n",
    "        'target_col': 'Kermit',\n",
    "        # 'model_name': 'RF',\n",
    "        # 'model': RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "        'model_name': 'DT',\n",
    "        'model': DecisionTreeClassifier(random_state=42)\n",
    "    },\n",
    "    {\n",
    "        'train_cols': [f'mfcc_{i}' for i in range(20)],\n",
    "        'target_col': 'Kermit',\n",
    "        # 'model_name': 'RF',\n",
    "        # 'model': RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "        'model_name': 'DT',\n",
    "        'model': DecisionTreeClassifier(random_state=42)\n",
    "    },\n",
    "    {\n",
    "        'train_cols': [f'percentage_{i}' for i in range(5)] + [f'hue_{i}' for i in range(5)],\n",
    "        'target_col': 'Kermit',\n",
    "        # 'model_name': 'RF',\n",
    "        # 'model': RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "        'model_name': 'DT',\n",
    "        'model': DecisionTreeClassifier(random_state=42)\n",
    "    },\n",
    "    {\n",
    "        'train_cols': ['zcr', 'rms'] + [f'mfcc_{i}' for i in range(20)] + [f'percentage_{i}' for i in range(5)] + [f'hue_{i}' for i in range(5)],\n",
    "        'target_col': 'Kermit',\n",
    "        # 'model_name': 'RF',\n",
    "        # 'model': RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "        'model_name': 'DT',\n",
    "        'model': DecisionTreeClassifier(random_state=42)\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b66232",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dfs = []\n",
    "model_infos = {}\n",
    "\n",
    "for train_config in train_configs:\n",
    "    config = ', '.join({re.search(r'(\\w+)_\\d+', c).group(1) if re.search(r'(\\w+)_\\d+', c) is not None else c for c in train_config['train_cols']})\n",
    "    print(config)\n",
    "    eval_df, model_info = train.train_eval_inner_cv(**train_config, config=config, feat_df=feat_df, gt_df=gt_df, episode_names=episode_names)\n",
    "\n",
    "    eval_dfs.append(eval_df)\n",
    "    model_infos[config] = model_info\n",
    "\n",
    "eval_df = pd.concat(eval_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c3daff",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a556bb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dfs = []\n",
    "model_infos = {}\n",
    "\n",
    "for train_config in train_configs:\n",
    "    config = ', '.join({re.search(r'(\\w+)_\\d+', c).group(1) if re.search(r'(\\w+)_\\d+', c) is not None else c for c in train_config['train_cols']})\n",
    "    print(config)\n",
    "    eval_df, model_info = train.train_eval_2_to_1(**train_config, config=config, feat_df=feat_df, gt_df=gt_df, episode_names=episode_names)\n",
    "\n",
    "    eval_dfs.append(eval_df)\n",
    "    model_infos[config] = model_info\n",
    "\n",
    "eval_df = pd.concat(eval_dfs)\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9880915",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.to_csv(\"../data/eval/DT_Kermit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870cf98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = {\n",
    "        'train_cols': [f'percentage_{i}' for i in range(5)] + [f'hue_{i}' for i in range(5)],\n",
    "        'target_col': 'Kermit',\n",
    "        'model_name': 'DecisionTree',\n",
    "        'model': DecisionTreeClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "config = ', '.join({re.search(r'(\\w+)_\\d+', c).group(1) if re.search(r'(\\w+)_\\d+', c) is not None else c for c in train_config['train_cols']})\n",
    "eval_df, model_info = train.train_eval_inner_cv(**train_config, config=config, feat_df=feat_df, gt_df=gt_df, episode_names=episode_names)\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09835527",
   "metadata": {},
   "source": [
    "## Gents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024e8836",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_configs = [\n",
    "    {\n",
    "        'train_cols': [f'percentage_{i}' for i in range(5)] + [f'hue_{i}' for i in range(5)],\n",
    "        'target_col': 'Audio_StatlerWaldorf',\n",
    "        'model_name': 'DecisionTree',\n",
    "        'model': DecisionTreeClassifier(random_state=42)\n",
    "    },\n",
    "    {\n",
    "        'train_cols': ['zcr', 'rms'] + [f'mfcc_{i}' for i in range(20)],\n",
    "        'target_col': 'Audio_StatlerWaldorf',\n",
    "        'model_name': 'DecisionTree',\n",
    "        'model': DecisionTreeClassifier(random_state=42)\n",
    "    },\n",
    "    {\n",
    "        'train_cols': [f'percentage_{i}' for i in range(5)] + [f'hue_{i}' for i in range(5)] + ['zcr', 'rms'] + [f'mfcc_{i}' for i in range(20)],\n",
    "        'target_col': 'Audio_StatlerWaldorf',\n",
    "        'model_name': 'DecisionTree',\n",
    "        'model': DecisionTreeClassifier(random_state=42)\n",
    "    },\n",
    "    {\n",
    "        'train_cols': [f'percentage_{i}' for i in range(5)] + [f'hue_{i}' for i in range(5)],\n",
    "        'target_col': 'StatlerWaldorf',\n",
    "        'model_name': 'DecisionTree',\n",
    "        'model': DecisionTreeClassifier(random_state=42)\n",
    "    },\n",
    "    {\n",
    "        'train_cols': ['zcr', 'rms'] + [f'mfcc_{i}' for i in range(20)],\n",
    "        'target_col': 'StatlerWaldorf',\n",
    "        'model_name': 'DecisionTree',\n",
    "        'model': DecisionTreeClassifier(random_state=42)\n",
    "    },\n",
    "    {\n",
    "        'train_cols': [f'percentage_{i}' for i in range(5)] + [f'hue_{i}' for i in range(5)] + ['zcr', 'rms'] + [f'mfcc_{i}' for i in range(20)],\n",
    "        'target_col': 'StatlerWaldorf',\n",
    "        'model_name': 'DecisionTree',\n",
    "        'model': DecisionTreeClassifier(random_state=42)\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83162929",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dfs = []\n",
    "model_infos = {}\n",
    "\n",
    "for train_config in train_configs:\n",
    "    config = ', '.join({re.search(r'(\\w+)_\\d+', c).group(1) if re.search(r'(\\w+)_\\d+', c) is not None else c for c in train_config['train_cols']})\n",
    "    print(config)\n",
    "    eval_df, model_info = train.train_eval_2_to_1(**train_config, config=config, feat_df=feat_df, gt_df=gt_df, episode_names=episode_names)\n",
    "\n",
    "    eval_dfs.append(eval_df)\n",
    "    model_infos[config] = model_info\n",
    "\n",
    "eval_df = pd.concat(eval_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414acec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c94ffcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.to_csv(\"../data/eval/DT_Gents.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b85a7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_dfs = []\n",
    "# model_infos = {}\n",
    "\n",
    "# for train_config in train_configs:\n",
    "#     config = ', '.join({re.search(r'(\\w+)_\\d+', c).group(1) if re.search(r'(\\w+)_\\d+', c) is not None else c for c in train_config['train_cols']})\n",
    "#     eval_df, model_info = train.train_eval_inner_cv(**train_config, config=config, feat_df=feat_df, gt_df=gt_df, episode_names=episode_names)\n",
    "\n",
    "#     eval_dfs.append(eval_df)\n",
    "#     model_infos[config] = model_info\n",
    "\n",
    "# eval_df = pd.concat(eval_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b4d562",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3b55ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = {\n",
    "        'train_cols': [f'percentage_{i}' for i in range(5)] + [f'hue_{i}' for i in range(5)],\n",
    "        'target_col': 'StatlerWaldorf',\n",
    "        'model_name': 'DecisionTree',\n",
    "        'model': DecisionTreeClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "config = ', '.join({re.search(r'(\\w+)_\\d+', c).group(1) if re.search(r'(\\w+)_\\d+', c) is not None else c for c in train_config['train_cols']})\n",
    "eval_df, model_info = train.train_eval_inner_cv(**train_config, config=config, feat_df=feat_df, gt_df=gt_df, episode_names=episode_names)\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fab10b",
   "metadata": {},
   "source": [
    "# Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa9852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# episode_id = 2\n",
    "# cap = io.load_video(video_paths[episode_names[episode_id]])\n",
    "\n",
    "# frames = []\n",
    "# for i in range(1000):\n",
    "#     ret, frame = cap.read()\n",
    "#     frames.append(frame)\n",
    "\n",
    "# img_orig, color = get_dominant_color(frames[942])\n",
    "\n",
    "# dominant_color_normalized = [x/255 for x in color]\n",
    "\n",
    "# # Create a new image of size 100x100 pixels and set all its pixels to the dominant color\n",
    "# image = np.full((100, 100, 3), dominant_color_normalized)\n",
    "\n",
    "# # Display the image\n",
    "# plt.imshow(image)\n",
    "\n",
    "# plt.imshow(img_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99aa3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0a725f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls '../data/frames/Muppets-02-04-04/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4074fedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.exists('../data/frames/Muppets-02-04-04/frame_06246.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24daf323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "img_path = '../data/frames/Muppets-03-04-03/frame_09302.jpg' \n",
    "img = cv.imread(img_path, cv.IMREAD_GRAYSCALE)\n",
    "assert img is not None, \"file could not be read, check with os.path.exists()\"\n",
    "laplacian = cv.Laplacian(img,cv.CV_64F)\n",
    "sobelx = cv.Sobel(img,cv.CV_64F,1,0,ksize=5)\n",
    "sobely = cv.Sobel(img,cv.CV_64F,0,1,ksize=5)\n",
    "plt.subplot(2,2,1),plt.imshow(img,cmap = 'gray')\n",
    "plt.title('Original'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(2,2,2),plt.imshow(laplacian,cmap = 'gray')\n",
    "plt.title('Laplacian'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(2,2,3),plt.imshow(sobelx,cmap = 'gray')\n",
    "plt.title('Sobel X'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(2,2,4),plt.imshow(sobely,cmap = 'gray')\n",
    "plt.title('Sobel Y'), plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb0f62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread(img_path, cv.IMREAD_GRAYSCALE)\n",
    "img[abs(sobely) < 100] = 0.0\n",
    "plt.subplot(2,2,4),plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52d3e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import cv2 as cv\n",
    "img_path = '../data/frames/Muppets-03-04-03/frame_09404.jpg' \n",
    "src = cv.imread(img_path, cv.IMREAD_GRAYSCALE)    \n",
    "dst = cv.Canny(src, 50, 200, None, 3)\n",
    "    \n",
    "# Copy edges to the images that will display the results in BGR\n",
    "cdst = cv.cvtColor(dst, cv.COLOR_GRAY2BGR)\n",
    "cdstP = np.copy(cdst)\n",
    "\n",
    "\n",
    "lines = cv.HoughLinesWithAccumulator(dst, 1, np.pi / 180, 200, None, 0, 0)\n",
    "\n",
    "pt1s = []\n",
    "pt2s = []\n",
    "\n",
    "if lines is not None:\n",
    "    for i in range(0, len(lines)):\n",
    "        rho = lines[i][0][0]\n",
    "        theta = lines[i][0][1]\n",
    "        a = math.cos(theta)\n",
    "        b = math.sin(theta)\n",
    "        x0 = a * rho\n",
    "        y0 = b * rho\n",
    "        pt1 = (int(x0 + 1000*(-b)), int(y0 + 1000*(a)))\n",
    "        pt2 = (int(x0 - 1000*(-b)), int(y0 - 1000*(a)))\n",
    "        pt1s.append(pt1)\n",
    "        pt2s.append(pt2)\n",
    "        cv.line(cdst, pt1, pt2, (0,0,255), 3, cv.LINE_AA)\n",
    "        if i == 3:\n",
    "            break\n",
    "\n",
    "\n",
    "# linesP = cv.HoughLinesP(dst, 0.5, np.pi / 10, 150, None, 50, 10)\n",
    "\n",
    "print(len(linesP))\n",
    "\n",
    "if linesP is not None:\n",
    "    for i in range(0, len(linesP)):\n",
    "        l = linesP[i][0]\n",
    "        cv.line(cdstP, (l[0], l[1]), (l[2], l[3]), (0,0,255), 3, cv.LINE_AA)\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(src)\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(cdst)\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(cdstP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.hlines([p[0] for p in pt1s], [p[1] for p in pt1s], c='orange')\n",
    "plt.hlines([p[0] for p in pt2s], [p[1] for p in pt2s], c='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
