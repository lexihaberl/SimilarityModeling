{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4c0604",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d64689b-26a5-4720-8ab8-6b7c41294322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import librosa\n",
    "\n",
    "from utils import io\n",
    "from utils import feature_engineering as fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92a0533-d581-454e-9bcb-0c94d117e38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_folder = \"../data/gt_annotations\"\n",
    "episode_names = ['Muppets-02-01-01', 'Muppets-02-04-04', 'Muppets-03-04-03']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_df = io.get_gt_df(episode_names, gt_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5432362",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: use straight lines for the detection of Balcony Gentlemens, since Balcony has a very characteristical straight lines. Can we use Hough Transform in Sim1? \n",
    "\n",
    "We say we can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import cv2 as cv\n",
    "img_path = '../data/frames/frame_09404.jpg' \n",
    "src = cv.imread(img_path, cv.IMREAD_GRAYSCALE)    \n",
    "dst = cv.Canny(src, 50, 200, None, 3)\n",
    "    \n",
    "cdst = cv.cvtColor(dst, cv.COLOR_GRAY2BGR)\n",
    "cdstP = np.copy(cdst)\n",
    "\n",
    "lines = cv.HoughLinesWithAccumulator(dst, 1, np.pi / 180, 200, None, 0, 0)\n",
    "\n",
    "if lines is not None:\n",
    "    for i in range(0, len(lines)):\n",
    "        rho = lines[i][0][0]\n",
    "        theta = lines[i][0][1]\n",
    "        a = math.cos(theta)\n",
    "        b = math.sin(theta)\n",
    "        x0 = a * rho\n",
    "        y0 = b * rho\n",
    "        pt1 = (int(x0 + 1000*(-b)), int(y0 + 1000*(a)))\n",
    "        pt2 = (int(x0 - 1000*(-b)), int(y0 - 1000*(a)))\n",
    "        cv.line(cdst, pt1, pt2, (0,0,255), 3, cv.LINE_AA)\n",
    "        if i == 3:\n",
    "            break\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(src)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(cdst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another idea: try to detect green blobs by highlighting the \"Kermit\" colors with Gaussian filter. Use skimgage Determinant of Hessian blob detector. Take 3 largest blob radien as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radius_col = 2\n",
    "n_max = 3\n",
    "debug = True\n",
    "\n",
    "img_path = '../data/frames/frame_10934.jpg'\n",
    "image = cv2.imread(img_path)\n",
    "image_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "image_rbg = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "if debug:\n",
    "    plt.imshow(image_rbg)\n",
    "\n",
    "sigma = 7\n",
    "green_yellow = np.uint8([[[255*0.85, 255, 0]]])\n",
    "hsv_green_yellow = cv2.cvtColor(green_yellow, cv2.COLOR_RGB2HSV)\n",
    "mu = hsv_green_yellow[0, 0, 0]\n",
    "\n",
    "blob_list = fe.detect_blob(image_hsv, sigma, mu, debug=debug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another idea we had is to detect Kermit's eyes by highlighting the yellowish-grey colors with high values. Sometimes it works, but in most cases it fails. We want to try to improve it by limiting number of hues in the images by retrieving dominant hues with clustering and replacing the original image hues by the dominant hues based on their cluster assignment.\n",
    "\n",
    "TODO: use color from the color histogram for kermit blob detection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_params = {\n",
    "    'mu_v': 225, \n",
    "    'sigma_v': 35, \n",
    "    'mu_h': 40, \n",
    "    'sigma_h': 35\n",
    "}\n",
    "img_path = '../data/frames/frame_31720.jpg'\n",
    "image = cv2.imread(img_path)\n",
    "image_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "image_rbg = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "if debug:\n",
    "    plt.imshow(image_rbg)\n",
    "    plt.show()\n",
    "\n",
    "keypoints = fe.detect_blob_cv(image_gray, image, image_hsv, debug=debug, **white_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we try to get the dominant hues by clustering and replacing the original hues by the dominant hues based on their cluster assignments. Here we plot only the hue channel, but it can still be used in combination with original saturation and value.\n",
    "\n",
    "TODO: try to combine this with blob detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import matplotlib.image as img\n",
    "from scipy.cluster.vq import whiten\n",
    "\n",
    "img_path = '../data/frames/frame_37154.jpg'\n",
    "image = cv2.imread(img_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    " \n",
    "hue = image[:, :, 0].flatten()\n",
    "hue_std = hue.std()\n",
    "scaled_hue = np.expand_dims(whiten(hue), -1)\n",
    " \n",
    " \n",
    "n_clusters = 5\n",
    "batch_size = 32\n",
    "km = MiniBatchKMeans(n_clusters = n_clusters, batch_size=batch_size, n_init='auto').fit(scaled_hue)\n",
    "cluster_centers = km.cluster_centers_\n",
    "\n",
    "\n",
    "dominant_hues = []\n",
    "for cluster_center in cluster_centers:\n",
    "    hue_scaled = cluster_center[0]\n",
    " \n",
    "    dominant_hues.append(\n",
    "        hue_scaled * hue_std,\n",
    "    )\n",
    "\n",
    "hues = np.asarray(dominant_hues, dtype='uint8')\n",
    "\n",
    "percentage = np.asarray(np.unique(km.labels_, return_counts = True)[1], dtype='float32')\n",
    "percentage = percentage/(image.shape[0]*image.shape[1])\n",
    "\n",
    "dom = [[percentage[ix], hues[ix]] for ix in range(km.n_clusters)]\n",
    "dominance = sorted(dom, key=lambda x:x[0], reverse=True)\n",
    "print(dominance)\n",
    "\n",
    "image_cp = image.copy()\n",
    "\n",
    "labels_mask = km.labels_.reshape(image.shape[0], image.shape[1])\n",
    "for label in range(km.n_clusters):\n",
    "    image_cp[labels_mask == label, 0] = hues[label]\n",
    "\n",
    "img = image_cp.reshape(image.shape[0], -1, 3)\n",
    "img = img[:, :, 0]\n",
    "\n",
    "plt.imshow(img, cmap='grey')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also tried to detect the edges with OpenCV, but we did not figure out how to use it as a feature yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '../data/frames/frame_37154.jpg'\n",
    "img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "laplacian = cv2.Laplacian(img,cv2.CV_64F)\n",
    "sobelx = cv2.Sobel(img,cv2.CV_64F,1,0,ksize=5)\n",
    "sobely = cv2.Sobel(img,cv2.CV_64F,0,1,ksize=5)\n",
    "plt.subplot(2,2,1),plt.imshow(img,cmap = 'gray')\n",
    "plt.title('Original'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(2,2,2),plt.imshow(laplacian,cmap = 'gray')\n",
    "plt.title('Laplacian'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(2,2,3),plt.imshow(sobelx,cmap = 'gray')\n",
    "plt.title('Sobel X'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(2,2,4),plt.imshow(sobely,cmap = 'gray')\n",
    "plt.title('Sobel Y'), plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also try to compute basic audio features: RMS, ZCR and MFCC.\n",
    "\n",
    "This code cannot be run since it needs the original videos. Thus it is commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0be812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ep_dfs = []\n",
    "# for ep in episode_names:\n",
    "#     rec, sr = librosa.load(video_paths[ep], sr=None)\n",
    "\n",
    "#     frame_size_ms = 400\n",
    "#     hop_length = int(1/25 * sr)\n",
    "#     frame_length = int(frame_size_ms / 1000 * sr)\n",
    "    \n",
    "#     desired_len = len(gt_df[gt_df.episode==ep])\n",
    "#     zcr = librosa.feature.zero_crossing_rate(y=rec, frame_length=frame_length, hop_length=hop_length)\n",
    "#     zcr = np.pad(zcr, pad_width=((0, 0), (0, desired_len - zcr.shape[1]))).flatten()\n",
    "\n",
    "#     rms = librosa.feature.rms(y=rec, frame_length=frame_length, hop_length=hop_length)\n",
    "#     rms = np.pad(rms, pad_width=((0, 0), (0, desired_len - rms.shape[1]))).flatten()\n",
    "\n",
    "#     mfcc = librosa.feature.mfcc(y=rec, sr=sr, n_fft=frame_length, hop_length=hop_length)\n",
    "#     mfcc = np.pad(mfcc, pad_width=((0, 0), (0, desired_len - mfcc.shape[1])))\n",
    "\n",
    "#     ep_df = pd.DataFrame()\n",
    "\n",
    "#     ep_df['zcr'] = zcr\n",
    "#     ep_df['rms'] = rms\n",
    "    \n",
    "#     for i in range(mfcc.shape[0]):\n",
    "#         ep_df[f'mfcc_{i}'] = mfcc[i]\n",
    "    \n",
    "#     ep_df['episode'] = ep\n",
    "#     ep_dfs.append(ep_df)\n",
    "\n",
    "# feat_df = pd.concat(ep_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b31404",
   "metadata": {},
   "source": [
    "# Training & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split Thoughts\n",
    "We decided to use nested CV for hyperparameter optimization (inner loop) and best models comparison (outer loop). By doing the nested CV we also don't have as much of a problem with e.g. the Swedish Chef being only in a small amount of scenes (which could lead to the chef being only represented in the test fold if we just do holdout for the test data). \n",
    "\n",
    "We are not completly sure how to split the episodes into multiple parts. Ofcourse we would try to create roughly equally-sized parts, while making sure that the cuts are not in the middle of a scene. Would it be ok to just split each episode in ~2-4 parts, so that we can create train-valid-splits with a ratio of e.g. 2/3-1/6-1/6?. This way we would mostly have data from each episode in the training set, which should improve the generalizability of the classifiers.\n",
    "\n",
    "### First try in training and evaluation\n",
    "For now, we have tried to classify using audio features and hue percentages (together and separately) with a DecisionTree. We can see that the results for Kermit classification are much better. Audio features are much more useful. The classification for gentlemen is still pretty bad. We hope to improve it by integrating the blob & line features.\n",
    "\n",
    "Here we used full episodes for training and evaluation. We trained on 2 episodes (train_fold) and tested on 1 episode (test_fold). The result very much depends on which fold is being tested.\n",
    "\n",
    "We did not do any hyperparameter optimisation yet, so the result will hopefully improve after we find optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kermit_eval_df = pd.read_csv(\"../data/eval/DT_Kermit.csv\")\n",
    "kermit_eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gents_eval_df = pd.read_csv(\"../data/eval/DT_Gents.csv\")\n",
    "gents_eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0ca4da",
   "metadata": {},
   "source": [
    "### Decided procedure\n",
    "2 CV loops.\n",
    "\n",
    "Outer CV: 5/6 (train in outer CV, train-val in inner CV), 1/6 (test)\n",
    "Inner CV: 4/6 (train), 1/6 (val) = 5/6 (train in outer CV)\n",
    "\n",
    "5-fold CV in inner CV. Different model types (RF, KNN) and different model hyperparams (RF1, RF2, ...), (KNN1, KNN2, ...). For each model type (RF, KNN) choose best hyperparameters by choosing params with best metrics on validation fold. \n",
    "For outer CV, train model with the chosen params on the train split, compute metrics on test split. Average metrics for each model type, choose model type with the best metrics on the test split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7714b5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
